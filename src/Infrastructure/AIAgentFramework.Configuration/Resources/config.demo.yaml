LLM:
  DefaultProvider: "openai"
  Providers:
    openai:
      ApiKey: "your-openai-api-key"
      Model: "gpt-4"
      MaxTokens: 4000
      Temperature: 0.7
    claude:
      ApiKey: "your-claude-api-key"
      Model: "claude-3-sonnet"
      MaxTokens: 4000
      Temperature: 0.7

Tools:
  BuiltIn:
    - Name: "embedding_cache"
      Enabled: true
    - Name: "vector_db"
      Enabled: true
  
  Plugins:
    Directory: "./plugins"
    AutoLoad: true
  
  MCP:
    Servers:
      - Name: "filesystem"
        Command: "npx"
        Args: ["@modelcontextprotocol/server-filesystem", "/tmp"]

Prompts:
  Directory: "./prompts"
  CacheTTL: 300
  DefaultLanguage: "ko"

UI:
  Web:
    Port: 5000
    SwaggerEnabled: true
  Console:
    BatchMode: false
    LogLevel: "Information"

Orchestration:
  MaxExecutionSteps: 20
  ExecutionTimeoutMinutes: 10
  SessionExpirationHours: 24