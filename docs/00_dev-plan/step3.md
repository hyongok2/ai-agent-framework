# ğŸ“ 3ë‹¨ê³„: LLM ê³„ì¸µ (LLM Layer)

---

## ğŸ¯ ëª©í‘œ

* LLM í˜¸ì¶œì„ **CallType ë‹¨ìœ„**ë¡œ í‘œì¤€í™”
* ê³µê¸‰ì(OpenAI, Claude, Ollama ë“±)ë³„ API ì°¨ì´ë¥¼ í¡ìˆ˜ â†’ ê°™ì€ ì¶”ìƒí™” ì¸í„°í˜ì´ìŠ¤ ì œê³µ
* **Prompt + Schema + Model Profile** ì¡°í•©ìœ¼ë¡œ ì™„ì „í•œ í˜¸ì¶œ ìŠ¤í™ êµ¬ì„±
* **Streaming ì´ë²¤íŠ¸** ê¸°ë°˜ ì‘ë‹µ êµ¬ì¡°í™”

---

## ğŸ“¦ ì£¼ìš” êµ¬ì„± ìš”ì†Œ

### 1. CallType (LLM í˜¸ì¶œ ìœ í˜•)

* ìš©ë„ì— ë”°ë¼ ì„¸ë¶„í™”ëœ í˜¸ì¶œ íƒ€ì… ì •ì˜

```csharp
public enum LlmCallType {
    ChatGenerate,      // ì¼ë°˜ ëŒ€í™”/í…ìŠ¤íŠ¸ ìƒì„±
    ChatJSON,          // JSON Schema ê¸°ë°˜ êµ¬ì¡°í™” ì‘ë‹µ
    Plan,              // ì‹¤í–‰ í”Œëœ(JSON) ìƒì„±
    ToolCallSuggest,   // í•¨ìˆ˜/ë„êµ¬ í˜¸ì¶œ ì œì•ˆ
    ToolCallLoop,      // ë„êµ¬ í˜¸ì¶œ ë£¨í”„
    Rerank,            // í›„ë³´ ì¬ìˆœìœ„
    Judge,             // í‰ê°€/ì±„ì 
    VisionQA,          // ì´ë¯¸ì§€ ì…ë ¥ Q&A
    VisionExtract,     // ì´ë¯¸ì§€â†’JSON ì¶”ì¶œ
    Embed,             // ë²¡í„° ì„ë² ë”©
    Reasoning,         // ê³ ë„ ì¶”ë¡ 
    CodeGen            // ì½”ë“œ ìƒì„±
}
```

---

### 2. Capabilities (ëª¨ë¸ ëŠ¥ë ¥ í”Œë˜ê·¸)

* ëª¨ë¸ë³„ ì§€ì› ë²”ìœ„ ì°¨ì´ë¥¼ í”Œë˜ê·¸ë¡œ ìº¡ìŠí™”

```csharp
[Flags]
public enum LlmCapabilities {
    StreamTokens = 1 << 0,
    JsonSchema   = 1 << 1,
    ToolUse      = 1 << 2,
    Vision       = 1 << 3,
    AudioIn      = 1 << 4,
    AudioOut     = 1 << 5,
    Reasoning    = 1 << 6,
    Batch        = 1 << 7,
    LongContext  = 1 << 8
}
```

---

### 3. LLM Request / Options

* ëª¨ë“  í˜¸ì¶œì€ **CallType + Prompt + Options** ì¡°í•©ìœ¼ë¡œ êµ¬ì„±

```csharp
public sealed record LlmRequest(
    LlmCallType CallType,
    PromptSpec Prompt,
    LlmOptions Options
);

public sealed record LlmOptions(
    string? ModelProfile = null,   // fast / analytic / local ë“±
    double? Temperature = null,
    int? MaxOutputTokens = null,
    TimeSpan? Deadline = null,
    string[]? Guardrails = null
);

public sealed record PromptSpec(
    string PromptTemplateId,       // prompts/plan/agent-plan.hbs
    IDictionary<string, object> Variables,
    string? SchemaRef = null       // schemas/AgentPlan.schema.json
);
```

---

### 4. LLM Chunk (Streaming ì‘ë‹µ)

* LLM ì‘ë‹µë„ ìŠ¤íŠ¸ë¦¬ë° ë‹¨ìœ„(`LlmChunk`)ë¡œ í˜ëŸ¬ë‚˜ì˜´

```csharp
public abstract record LlmChunk(RunId RunId, StepId StepId, long Seq);

public sealed record LlmTokenChunk(RunId RunId, StepId StepId, long Seq, string Text) : LlmChunk(RunId, StepId, Seq);
public sealed record LlmJsonPartialChunk(RunId RunId, StepId StepId, long Seq, string PartialJson) : LlmChunk(RunId, StepId, Seq);
public sealed record LlmToolCallChunk(RunId RunId, StepId StepId, long Seq, string ToolName, JsonNode Args) : LlmChunk(RunId, StepId, Seq);
public sealed record LlmFinalChunk(RunId RunId, StepId StepId, long Seq, JsonNode Result) : LlmChunk(RunId, StepId, Seq);
```

---

### 5. ILlmClient (í‘œì¤€ ì¸í„°í˜ì´ìŠ¤)

* ëª¨ë“  LLM Providerê°€ êµ¬í˜„í•´ì•¼ í•˜ëŠ” ê³„ì•½

```csharp
public interface ILlmClient
{
    IAsyncEnumerable<LlmChunk> CompleteAsync(LlmRequest request, CancellationToken ct);
    LlmCapabilities Capabilities { get; }
}
```

---

### 6. Model Profiles (ë¼ìš°íŒ… ë‹¨ìœ„)

* `configs/model-profiles/` ë””ë ‰í† ë¦¬ì— JSONìœ¼ë¡œ ì •ì˜
* ì˜ˆì‹œ: `fast.json`

```json
{
  "description": "ë¹ ë¥¸ ì‘ë‹µìš© ì†Œí˜• ëª¨ë¸",
  "provider": "openai",
  "model": "gpt-4o-mini",
  "temperature": 0.7,
  "maxOutputTokens": 512
}
```

* ì˜ˆì‹œ: `local.json`

```json
{
  "description": "ë¡œì»¬ GPU ëª¨ë¸",
  "provider": "ollama",
  "model": "qwen2.5:14b-instruct-q5",
  "endpoint": "http://localhost:11434",
  "temperature": 0.2
}
```

---

### 7. Router

* `(CallType, ModelProfile)`ì„ ë³´ê³  ì‹¤ì œ ëª¨ë¸ì„ ì„ íƒ
* Capabilities ê²€ì‚¬ í›„ ë¶ˆì¼ì¹˜ ì‹œ fallback ì²˜ë¦¬

```csharp
public interface ILlmRouter
{
    ILlmClient Resolve(LlmRequest request);
}
```

---

### 8. Prompt Engine

* í…œí”Œë¦¿(`.liquid` ë˜ëŠ” `.scriban`) + ë³€ìˆ˜ ë°”ì¸ë”© â†’ ìµœì¢… ë¬¸ìì—´
* SchemaRef ìˆëŠ” ê²½ìš° â†’ "JSON only" directive ìë™ ì‚½ì…
* Few-shot ì˜ˆì‹œ ì§€ì› (`samples/fewshots/â€¦`)

```csharp
public interface IPromptEngine
{
    string Render(PromptSpec spec);
}
```

---

### 9. Schema Validator (ì—°ê³„)

* `ChatJSON`, `Plan`, `ToolCallSuggest` ë“±ì€ **Schema ê²€ì¦ í•„ìˆ˜**
* Autofix ëª¨ë“œ: LLM ì‘ë‹µì´ ì˜ëª»ëœ JSONì¼ ê²½ìš° â†’ ë‹¤ì‹œ LLMì— ìˆ˜ì • ìš”ì²­

---

## ğŸ“‚ ë””ë ‰í† ë¦¬ ë°°ì¹˜ (3ë‹¨ê³„ ì‚°ì¶œë¬¼)

```
src/Agent.Llm.Abstractions/
  ILlmClient.cs
  LlmRequest.cs
  LlmOptions.cs
  LlmChunk.cs
  LlmCapabilities.cs
  LlmCallType.cs
  ILlmRouter.cs
  IPromptEngine.cs

src/Agent.Llm.OpenAI/
  OpenAiClient.cs
  OpenAiPromptEngine.cs

src/Agent.Llm.Claude/
  ClaudeClient.cs

src/Agent.Llm.Ollama/
  OllamaClient.cs

configs/model-profiles/
  fast.json
  analytic.json
  local.json
  secure.json

samples/prompts/
  plan/agent-plan.liquid
  tool/suggest.liquid
  extract/fields.liquid

samples/fewshots/
  classify/colors.json
  summarize/meeting.json
```

---

## âœ… 3ë‹¨ê³„ ì™„ë£Œ ê¸°ì¤€

* [ ] `LlmCallType`, `LlmCapabilities` ì •ì˜
* [ ] `ILlmClient`, `ILlmRouter`, `IPromptEngine` ê³„ì•½ í™•ì •
* [ ] Model Profiles ìµœì†Œ 3ê°œ ì‘ì„± (`fast`, `analytic`, `local`)
* [ ] OpenAI Adapterì—ì„œ ìµœì†Œ 3 CallType ë™ì‘ (`ChatGenerate`, `ChatJSON`, `Plan`)
* [ ] Claude Adapterì—ì„œ `ToolCallSuggest` + `ToolCallLoop` êµ¬í˜„
* [ ] Ollama Adapterì—ì„œ `ChatGenerate` êµ¬í˜„
* [ ] Prompt Engineì—ì„œ `.liquid` í…œí”Œë¦¿ ë¡œë”© ë° ë³€ìˆ˜ ì¹˜í™˜ í…ŒìŠ¤íŠ¸ ì„±ê³µ
* [ ] Schema Validatorë¡œ JSON ì¶œë ¥ ê°•ì œ í…ŒìŠ¤íŠ¸ ì„±ê³µ
