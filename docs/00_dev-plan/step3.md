# ğŸ“ 3ë‹¨ê³„: LLM ê³„ì¸µ (LLM Layer)

---

## ğŸ¯ ëª©í‘œ

* LLM í˜¸ì¶œì„ **CallType ë‹¨ìœ„**ë¡œ í‘œì¤€í™”
* ê³µê¸‰ì(OpenAI, Claude, Ollama ë“±)ë³„ API ì°¨ì´ë¥¼ í¡ìˆ˜ â†’ ê°™ì€ ì¶”ìƒí™” ì¸í„°í˜ì´ìŠ¤ ì œê³µ
* **Prompt + Schema + Model Profile** ì¡°í•©ìœ¼ë¡œ ì™„ì „í•œ í˜¸ì¶œ ìŠ¤í™ êµ¬ì„±
* **Streaming ì´ë²¤íŠ¸** ê¸°ë°˜ ì‘ë‹µ êµ¬ì¡°í™”

---

## ğŸ“¦ ì£¼ìš” êµ¬ì„± ìš”ì†Œ

### 1. CallType (LLM í˜¸ì¶œ ìœ í˜•)

* ìš©ë„ì— ë”°ë¼ ì„¸ë¶„í™”ëœ í˜¸ì¶œ íƒ€ì… ì •ì˜

```csharp
public enum LlmCallType {
    ChatGenerate,      // ì¼ë°˜ ëŒ€í™”/í…ìŠ¤íŠ¸ ìƒì„±
    ChatJSON,          // JSON Schema ê¸°ë°˜ êµ¬ì¡°í™” ì‘ë‹µ
    Plan,              // ì‹¤í–‰ í”Œëœ(JSON) ìƒì„±
    ToolCallSuggest,   // í•¨ìˆ˜/ë„êµ¬ í˜¸ì¶œ ì œì•ˆ
    ToolCallLoop,      // ë„êµ¬ í˜¸ì¶œ ë£¨í”„
    Rerank,            // í›„ë³´ ì¬ìˆœìœ„
    Judge,             // í‰ê°€/ì±„ì 
    VisionQA,          // ì´ë¯¸ì§€ ì…ë ¥ Q&A
    VisionExtract,     // ì´ë¯¸ì§€â†’JSON ì¶”ì¶œ
    Embed,             // ë²¡í„° ì„ë² ë”©
    Reasoning,         // ê³ ë„ ì¶”ë¡ 
    CodeGen            // ì½”ë“œ ìƒì„±
}
```

---

### 2. Capabilities (ëª¨ë¸ ëŠ¥ë ¥ í”Œë˜ê·¸)

* ëª¨ë¸ë³„ ì§€ì› ë²”ìœ„ ì°¨ì´ë¥¼ í”Œë˜ê·¸ë¡œ ìº¡ìŠí™”

```csharp
[Flags]
public enum LlmCapabilities {
    StreamTokens = 1 << 0,
    JsonSchema   = 1 << 1,
    ToolUse      = 1 << 2,
    Vision       = 1 << 3,
    AudioIn      = 1 << 4,
    AudioOut     = 1 << 5,
    Reasoning    = 1 << 6,
    Batch        = 1 << 7,
    LongContext  = 1 << 8
}
```

---

### 3. LLM Request / Options

* ëª¨ë“  í˜¸ì¶œì€ **CallType + Prompt + Options** ì¡°í•©ìœ¼ë¡œ êµ¬ì„±

```csharp
public sealed record LlmRequest(
    LlmCallType CallType,
    PromptSpec Prompt,
    LlmOptions Options
);

public sealed record LlmOptions(
    string? ModelProfile = null,   // fast / analytic / local ë“±
    double? Temperature = null,
    int? MaxOutputTokens = null,
    TimeSpan? Deadline = null,
    string[]? Guardrails = null
);

public sealed record PromptSpec(
    string PromptTemplateId,       // prompts/plan/agent-plan.hbs
    IDictionary<string, object> Variables,
    string? SchemaRef = null       // schemas/AgentPlan.schema.json
);
```

---

### 4. LLM Chunk (Streaming ì‘ë‹µ)

* LLM ì‘ë‹µë„ ìŠ¤íŠ¸ë¦¬ë° ë‹¨ìœ„(`LlmChunk`)ë¡œ í˜ëŸ¬ë‚˜ì˜´

```csharp
public abstract record LlmChunk(RunId RunId, StepId StepId, long Seq);

public sealed record LlmTokenChunk(RunId RunId, StepId StepId, long Seq, string Text) : LlmChunk(RunId, StepId, Seq);
public sealed record LlmJsonPartialChunk(RunId RunId, StepId StepId, long Seq, string PartialJson) : LlmChunk(RunId, StepId, Seq);
public sealed record LlmToolCallChunk(RunId RunId, StepId StepId, long Seq, string ToolName, JsonNode Args) : LlmChunk(RunId, StepId, Seq);
public sealed record LlmFinalChunk(RunId RunId, StepId StepId, long Seq, JsonNode Result) : LlmChunk(RunId, StepId, Seq);
```

---

### 5. ILlmClient (í‘œì¤€ ì¸í„°í˜ì´ìŠ¤)

* ëª¨ë“  LLM Providerê°€ êµ¬í˜„í•´ì•¼ í•˜ëŠ” ê³„ì•½

```csharp
public interface ILlmClient
{
    IAsyncEnumerable<LlmChunk> CompleteAsync(LlmRequest request, CancellationToken ct);
    LlmCapabilities Capabilities { get; }
}
```

---

### 6. Model Profiles (ë¼ìš°íŒ… ë‹¨ìœ„)

* `configs/model-profiles/` ë””ë ‰í† ë¦¬ì— JSONìœ¼ë¡œ ì •ì˜
* ì˜ˆì‹œ: `fast.json`

```json
{
  "description": "ë¹ ë¥¸ ì‘ë‹µìš© ì†Œí˜• ëª¨ë¸",
  "provider": "openai",
  "model": "gpt-4o-mini",
  "temperature": 0.7,
  "maxOutputTokens": 512
}
```

* ì˜ˆì‹œ: `local.json`

```json
{
  "description": "ë¡œì»¬ GPU ëª¨ë¸",
  "provider": "ollama",
  "model": "qwen2.5:14b-instruct-q5",
  "endpoint": "http://localhost:11434",
  "temperature": 0.2
}
```

---

### 7. Router

* `(CallType, ModelProfile)`ì„ ë³´ê³  ì‹¤ì œ ëª¨ë¸ì„ ì„ íƒ
* Capabilities ê²€ì‚¬ í›„ ë¶ˆì¼ì¹˜ ì‹œ fallback ì²˜ë¦¬

```csharp
public interface ILlmRouter
{
    ILlmClient Resolve(LlmRequest request);
}
```

---

### 8. Prompt Engine

* í…œí”Œë¦¿(`.liquid` ë˜ëŠ” `.scriban`) + ë³€ìˆ˜ ë°”ì¸ë”© â†’ ìµœì¢… ë¬¸ìì—´
* SchemaRef ìˆëŠ” ê²½ìš° â†’ "JSON only" directive ìë™ ì‚½ì…
* Few-shot ì˜ˆì‹œ ì§€ì› (`samples/fewshots/â€¦`)

```csharp
public interface IPromptEngine
{
    string Render(PromptSpec spec);
}
```

---

### 9. Schema Validator (ì—°ê³„)

* `ChatJSON`, `Plan`, `ToolCallSuggest` ë“±ì€ **Schema ê²€ì¦ í•„ìˆ˜**
* Autofix ëª¨ë“œ: LLM ì‘ë‹µì´ ì˜ëª»ëœ JSONì¼ ê²½ìš° â†’ ë‹¤ì‹œ LLMì— ìˆ˜ì • ìš”ì²­

---

## ğŸ“‚ ë””ë ‰í† ë¦¬ ë°°ì¹˜ (3ë‹¨ê³„ ì‚°ì¶œë¬¼)

```
src/Agent.Llm.Abstractions/
  ILlmClient.cs
  LlmRequest.cs
  LlmOptions.cs
  LlmChunk.cs
  LlmCapabilities.cs
  LlmCallType.cs
  ILlmRouter.cs
  IPromptEngine.cs

src/Agent.Llm.OpenAI/
  OpenAiClient.cs
  OpenAiPromptEngine.cs

src/Agent.Llm.Claude/
  ClaudeClient.cs

src/Agent.Llm.Ollama/
  OllamaClient.cs

configs/model-profiles/
  fast.json
  analytic.json
  local.json
  secure.json

samples/prompts/
  plan/agent-plan.liquid
  tool/suggest.liquid
  extract/fields.liquid

samples/fewshots/
  classify/colors.json
  summarize/meeting.json
```

---

## âœ… 3ë‹¨ê³„ ì™„ë£Œ ê¸°ì¤€

* [ ] `LlmCallType`, `LlmCapabilities` ì •ì˜
* [ ] `ILlmClient`, `ILlmRouter`, `IPromptEngine` ê³„ì•½ í™•ì •
* [ ] Model Profiles ìµœì†Œ 3ê°œ ì‘ì„± (`fast`, `analytic`, `local`)
* [ ] OpenAI Adapterì—ì„œ ìµœì†Œ 3 CallType ë™ì‘ (`ChatGenerate`, `ChatJSON`, `Plan`)
* [ ] Claude Adapterì—ì„œ `ToolCallSuggest` + `ToolCallLoop` êµ¬í˜„
* [ ] Ollama Adapterì—ì„œ `ChatGenerate` êµ¬í˜„
* [ ] Prompt Engineì—ì„œ `.liquid` í…œí”Œë¦¿ ë¡œë”© ë° ë³€ìˆ˜ ì¹˜í™˜ í…ŒìŠ¤íŠ¸ ì„±ê³µ
* [ ] Schema Validatorë¡œ JSON ì¶œë ¥ ê°•ì œ í…ŒìŠ¤íŠ¸ ì„±ê³µ



í˜„ì¬ í”„ë¡œì íŠ¸ êµ¬ì¡°ë¥¼ ë¶„ì„í•´ë³´ë‹ˆ, ì¶”ìƒí™” ê³„ì¸µì€ ì˜ êµ¬í˜„ë˜ì–´ ìˆì§€ë§Œ `step3.md`ì˜ LLM ê³„ì¸µ êµ¬í˜„ì—ì„œ ëª‡ ê°€ì§€ ë³´ê°•ì´ í•„ìš”í•œ ë¶€ë¶„ë“¤ì´ ìˆìŠµë‹ˆë‹¤.

## ğŸ“‹ LLM ê³„ì¸µ êµ¬í˜„ ë³´ê°• ì‚¬í•­

### 1. **êµ¬ì²´ì  êµ¬í˜„ì²´ ì‘ì„± í•„ìš”**

#### OpenAI í´ë¼ì´ì–¸íŠ¸ êµ¬í˜„
```csharp
// src/Agent.Llm.OpenAI/OpenAiClient.cs
- OpenAI API ì—°ë™ êµ¬í˜„
- GPT-4, GPT-3.5 ëª¨ë¸ ì§€ì›
- í•¨ìˆ˜ í˜¸ì¶œ ì§€ì›
- ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
- í† í° ì¹´ìš´íŒ… ë° ë¹„ìš© ê³„ì‚°
```

#### Claude í´ë¼ì´ì–¸íŠ¸ êµ¬í˜„
```csharp
// src/Agent.Llm.Claude/ClaudeClient.cs
- Anthropic API ì—°ë™
- Claude 3 ëª¨ë¸ ì§€ì›
- ë„êµ¬ ì‚¬ìš© íŒ¨í„´ êµ¬í˜„
- ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ê´€ë¦¬
```

#### Ollama í´ë¼ì´ì–¸íŠ¸ êµ¬í˜„
```csharp
// src/Agent.Llm.Ollama/OllamaClient.cs
- ë¡œì»¬ Ollama API ì—°ë™
- ëª¨ë¸ ìë™ íƒìƒ‰
- ìŠ¤íŠ¸ë¦¬ë° ì§€ì›
- ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§
```

### 2. **LLM Router êµ¬í˜„**

```csharp
// src/Agent.Llm.Core/Routing/DefaultLlmRouter.cs
í•„ìš” ê¸°ëŠ¥:
- CallTypeë³„ ìµœì  ëª¨ë¸ ì„ íƒ ë¡œì§
- Capabilities ê¸°ë°˜ fallback ì²˜ë¦¬
- ë¹„ìš© ìµœì í™” ë¼ìš°íŒ…
- ë¶€í•˜ ë¶„ì‚° ë¡œì§
- ì‹¤íŒ¨ ì‹œ ì¬ë¼ìš°íŒ…
```

### 3. **Prompt Engine êµ¬í˜„**

```csharp
// src/Agent.Llm.Core/Prompts/PromptEngine.cs
í•„ìš” ê¸°ëŠ¥:
- Liquid/Scriban í…œí”Œë¦¿ ì—”ì§„ í†µí•©
- ë³€ìˆ˜ ë°”ì¸ë”© ë° ê²€ì¦
- Few-shot ì˜ˆì œ ìë™ ì‚½ì…
- Schema ê¸°ë°˜ JSON ì§€ì‹œë¬¸ ì¶”ê°€
- í”„ë¡¬í”„íŠ¸ ìµœì í™” (í† í° ì ˆì•½)
- ë‹¤êµ­ì–´ í”„ë¡¬í”„íŠ¸ ì§€ì›
```

### 4. **Model Profile Manager**

```csharp
// src/Agent.Llm.Core/Profiles/ModelProfileManager.cs
í•„ìš” ê¸°ëŠ¥:
- JSON í”„ë¡œí•„ íŒŒì¼ ë¡œë”©
- í”„ë¡œí•„ ê²€ì¦ ë° ë³‘í•©
- í™˜ê²½ë³„ ì˜¤ë²„ë¼ì´ë“œ
- ë™ì  í”„ë¡œí•„ ì—…ë°ì´íŠ¸
- í”„ë¡œí•„ ë²„ì „ ê´€ë¦¬
```

### 5. **CallType í•¸ë“¤ëŸ¬**

ê° CallTypeë³„ ì „ë¬¸ í•¸ë“¤ëŸ¬ êµ¬í˜„:

```csharp
// src/Agent.Llm.Core/Handlers/
- ChatGenerateHandler.cs
- ChatJsonHandler.cs  
- PlanGenerationHandler.cs
- ToolCallSuggestHandler.cs
- ToolCallLoopHandler.cs
- ReasoningHandler.cs
- VisionQAHandler.cs
- EmbeddingHandler.cs
```

### 6. **ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ê°œì„ **

```csharp
// src/Agent.Llm.Core/Streaming/StreamProcessor.cs
í•„ìš” ê¸°ëŠ¥:
- ì²­í¬ ë²„í¼ë§ ë° ì¬ì¡°ë¦½
- JSON ë¶€ë¶„ íŒŒì‹±
- í•¨ìˆ˜ í˜¸ì¶œ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬
- ì—ëŸ¬ ë³µêµ¬ ë©”ì»¤ë‹ˆì¦˜
- ë°±í”„ë ˆì…” ì²˜ë¦¬
```

### 7. **JSON Schema í†µí•©**

```csharp
// src/Agent.Llm.Core/Schema/SchemaEnforcer.cs
í•„ìš” ê¸°ëŠ¥:
- LLM ì‘ë‹µ ê²€ì¦
- ìë™ ìˆ˜ì • ì‹œë„
- Schema â†’ TypeScript/Python íƒ€ì… ë³€í™˜
- OpenAPI ìŠ¤í‚¤ë§ˆ í˜¸í™˜ì„±
```

### 8. **ë©”íŠ¸ë¦­ ë° ëª¨ë‹ˆí„°ë§**

```csharp
// src/Agent.Llm.Core/Monitoring/LlmMetricsCollector.cs
í•„ìš” ê¸°ëŠ¥:
- í† í° ì‚¬ìš©ëŸ‰ ì¶”ì 
- ì‘ë‹µ ì‹œê°„ ì¸¡ì •
- ì—ëŸ¬ìœ¨ ëª¨ë‹ˆí„°ë§
- ë¹„ìš© ê³„ì‚° ë° ì˜ˆì‚° ê´€ë¦¬
- ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ
```

### 9. **ìºì‹± ë ˆì´ì–´**

```csharp
// src/Agent.Llm.Core/Caching/LlmCache.cs
í•„ìš” ê¸°ëŠ¥:
- ì‹œë§¨í‹± ìºì‹± (ì„ë² ë”© ê¸°ë°˜)
- ì •í™•í•œ ë§¤ì¹­ ìºì‹œ
- TTL ê´€ë¦¬
- ìºì‹œ ë¬´íš¨í™” ì •ì±…
```

### 10. **í…ŒìŠ¤íŠ¸ ë° Mock**

```csharp
// src/Agent.Llm.Tests/
í•„ìš” í•­ëª©:
- MockLlmClient (í…ŒìŠ¤íŠ¸ìš©)
- ê° CallTypeë³„ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
- í†µí•© í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤
- ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
- ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸
```

### 11. **ì„¤ì • íŒŒì¼ êµ¬ì¡°**

```yaml
# configs/model-profiles/fast.yaml
profile: fast
description: "ë¹ ë¥¸ ì‘ë‹µìš© ì†Œí˜• ëª¨ë¸"
providers:
  - name: openai
    model: gpt-4o-mini
    priority: 1
    settings:
      temperature: 0.7
      maxTokens: 512
  - name: claude
    model: claude-3-haiku
    priority: 2
fallback:
  enabled: true
  maxAttempts: 3
```

### 12. **í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì˜ˆì œ**

```liquid
# samples/prompts/plan/agent-plan.liquid
{% if system_context %}
System Context: {{ system_context }}
{% endif %}

Generate an execution plan for the following request:
{{ user_request }}

{% if constraints %}
Constraints:
{% for constraint in constraints %}
- {{ constraint }}
{% endfor %}
{% endif %}

Output Format: JSON following the schema {{ schema_ref }}
```

### 13. **ì˜¤ë¥˜ ì²˜ë¦¬ ë° ì¬ì‹œë„**

```csharp
// src/Agent.Llm.Core/Resilience/RetryPolicy.cs
í•„ìš” ê¸°ëŠ¥:
- ì§€ìˆ˜ ë°±ì˜¤í”„
- ì„œí‚· ë¸Œë ˆì´ì»¤
- íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬
- ë¶€ë¶„ ì‹¤íŒ¨ ë³µêµ¬
- ë°ë“œë ˆí„° í
```

### 14. **ë³´ì•ˆ ë° ê±°ë²„ë„ŒìŠ¤**

```csharp
// src/Agent.Llm.Core/Security/
- ContentFilter.cs (ìœ í•´ ì½˜í…ì¸  í•„í„°ë§)
- PiiRedactor.cs (ê°œì¸ì •ë³´ ë§ˆìŠ¤í‚¹)
- RateLimiter.cs (ì†ë„ ì œí•œ)
- AuditLogger.cs (ê°ì‚¬ ë¡œê¹…)
```

### 15. **ë¬¸ì„œí™”**

```markdown
í•„ìš” ë¬¸ì„œ:
- API ë ˆí¼ëŸ°ìŠ¤
- ê° CallType ì‚¬ìš© ê°€ì´ë“œ
- ëª¨ë¸ ì„ íƒ ê°€ì´ë“œ
- í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤
- ë¹„ìš© ìµœì í™” ê°€ì´ë“œ
- íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ê°€ì´ë“œ
```

ì´ëŸ¬í•œ êµ¬í˜„ ì‚¬í•­ë“¤ì„ ë‹¨ê³„ì ìœ¼ë¡œ ì§„í–‰í•˜ë˜, ìš°ì„ ìˆœìœ„ëŠ”:
1. **í•µì‹¬ í´ë¼ì´ì–¸íŠ¸ êµ¬í˜„** (OpenAI, Claude, Ollama)
2. **Routerì™€ Prompt Engine**
3. **CallType í•¸ë“¤ëŸ¬**
4. **ìŠ¤íŠ¸ë¦¬ë° ë° ìºì‹±**
5. **ëª¨ë‹ˆí„°ë§ ë° ë³´ì•ˆ**

ìˆœìœ¼ë¡œ ì§„í–‰í•˜ëŠ” ê²ƒì„ ì¶”ì²œí•©ë‹ˆë‹¤.